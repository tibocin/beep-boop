{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Agentic Companion Development Notebook\n",
    "\n",
    "**A step-by-step guide to building your RAG-powered Gradio agent**\n",
    "\n",
    "This notebook will walk you through creating an intelligent conversational agent with:\n",
    "- Request parsing and classification\n",
    "- RAG-powered context retrieval\n",
    "- Gradio web interface\n",
    "- Voice input/output capabilities\n",
    "- Learning and memory systems\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Define ReqPrompt & Enums](#enums)\n",
    "3. [Build Chat Function](#chat)\n",
    "4. [Create Gradio UI](#gradio)\n",
    "5. [Implement Request Parser](#parser)\n",
    "6. [Add Subject Agent](#agent)\n",
    "7. [YAML-Based RAG](#rag)\n",
    "8. [Chroma Integration](#chroma)\n",
    "9. [Voice Features](#voice)\n",
    "10. [Testing & Expansion](#testing)\n",
    "11. [Logging System](#logging)\n",
    "12. [Learning Mode](#learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 1. Setup & Imports <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n",
      "OpenAI API Key: ‚úÖ Set\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Core packages\n",
    "import openai\n",
    "import os\n",
    "import gradio as gr\n",
    "import yaml\n",
    "import sqlite3\n",
    "from typing import List, Dict, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# üß† Vector + Embeddings\n",
    "# import chromadb # noqa: F401\n",
    "# from chromadb.config import Settings # noqa: F401\n",
    "\n",
    "# üåê Keys and config\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"OpenAI API Key: {'‚úÖ Set' if openai.api_key else '‚ùå Missing'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 2. Define ReqPrompt & Enums <a name=\"enums\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enums and ReqPrompt defined!\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class Subject(Enum):\n",
    "    PERSONALITY = \"personality\"\n",
    "    PROJECTS = \"projects\"\n",
    "    VALUES = \"values\"\n",
    "    TECHNICAL_SKILLS = \"technical_skills\"\n",
    "    GENERAL = \"general\"\n",
    "    EDUCATION = \"education\"\n",
    "    INTERESTS = \"interests\"\n",
    "    PRODUCT_FEATURES = \"product_features\"\n",
    "    BUSINESS_IDEAS = \"business_ideas\"\n",
    "    WORK_EXPERIENCE = \"work_experience\"\n",
    "    FAVORITES = \"favorites\"\n",
    "    LIFESTYLE = \"lifestyle\"\n",
    "    FAMILY = \"family\"\n",
    "    PARADIGMS = \"paradigms\"\n",
    "    RELATIONSHIPS = \"relationships\"\n",
    "    ROMANCE = \"romance\"\n",
    "    SPIRITUALITY = \"spirituality\"\n",
    "    RELIGION = \"religion\"\n",
    "    PHILOSOPHY = \"philosophy\"\n",
    "    ETHICS = \"ethics\"\n",
    "    POLITICS = \"politics\"\n",
    "    ECONOMICS = \"economics\"\n",
    "    ACTIVITIES = \"activities\"\n",
    "\n",
    "class Format(Enum):\n",
    "    BACKGROUND = \"background\"\n",
    "    PROBLEM_SOLVE = \"problem_solve\"\n",
    "    EXPLANATION = \"explanation\"\n",
    "    ETHICAL_DILEMMA = \"ethical_dilemma\"\n",
    "    VALUE_ASSESSMENT = \"value_assessment\"\n",
    "    PLANNING = \"planning\"\n",
    "    RESEARCH = \"research\"\n",
    "    REVIEW = \"review\"\n",
    "    STORY = \"story\"\n",
    "    QUESTION = \"question\"\n",
    "    DATA = \"data\"\n",
    "    ANALOGY = \"analogy\"\n",
    "    METAPHOR = \"metaphor\"\n",
    "    SYMBOLIC = \"symbolic\"\n",
    "\n",
    "class Tone(Enum):\n",
    "    PROFESSIONAL = \"professional\"\n",
    "    POETIC = \"poetic\"\n",
    "    CASUAL = \"casual\"\n",
    "    TECHNICAL = \"technical\"\n",
    "    FORMAL = \"formal\"\n",
    "    SHAMANIC_ESOTERIC = \"shamanic_esoteric\"\n",
    "    PASSIONATE = \"passionate\"\n",
    "    MATTER_OF_FACT = \"matter_of_fact\"\n",
    "    NOETIC = \"noetic\"\n",
    "    HUMOROUS = \"humorous\"\n",
    "    WITTY = \"witty\"\n",
    "    CONTEMPLATIVE = \"contemplative\"\n",
    "\n",
    "class OutputStyle(Enum):\n",
    "    CONCISE = \"concise\"\n",
    "    STORYTELLING = \"storytelling\"\n",
    "    DETAILED = \"detailed\"\n",
    "    BULLET_POINTS = \"bullet_points\"\n",
    "    THOUGHT_PROVOKING = \"thought_provoking\"\n",
    "    CONVERSATIONAL = \"conversational\"\n",
    "    DEVOTIONAL = \"devotional\"\n",
    "    CODE = \"code\"\n",
    "    DATA = \"data\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReqPrompt:\n",
    "    subject: Subject\n",
    "    format: Format\n",
    "    tone: Tone\n",
    "    style: OutputStyle\n",
    "    score: float  # Confidence score 0-1\n",
    "    feedback: str\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"{self.subject.value} | {self.format.value} | {self.tone.value} | {self.style.value} | Score: {self.score:.2f}\"\n",
    "\n",
    "print(\"‚úÖ Enums and ReqPrompt defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí¨ 3. Build Chat Function <a name=\"chat\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test response: Echo: Hello, how are you? (replace this with actual agent response)\n",
      "‚úÖ Chat function created!\n"
     ]
    }
   ],
   "source": [
    "def chat_fn(message: str, history: List[List[str]] = None) -> str:\n",
    "    \"\"\"\n",
    "    Main chat function that processes user messages and returns responses.\n",
    "    \n",
    "    Args:\n",
    "        message: User's input message\n",
    "        history: Previous conversation history\n",
    "    \n",
    "    Returns:\n",
    "        str: Agent's response\n",
    "    \"\"\"\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # TODO: Replace with actual agent logic\n",
    "    response = f\"Echo: {message} (replace this with actual agent response)\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the function\n",
    "test_response = chat_fn(\"Hello, how are you?\")\n",
    "print(f\"Test response: {test_response}\")\n",
    "print(\"‚úÖ Chat function created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñº 4. Create Gradio UI <a name=\"gradio\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/81/r1rhv_492c331qvk5jd7xtdh0000gn/T/ipykernel_95022/1021313803.py:14: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradio interface created!\n"
     ]
    }
   ],
   "source": [
    "# Create the Gradio interface\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Create and return the Gradio interface for the agentic companion.\"\"\"\n",
    "    \n",
    "    with gr.Blocks(title=\"üß† Agentic Companion\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # üß† Agentic Companion\n",
    "        \n",
    "        Your intelligent conversational agent with RAG-powered knowledge and learning capabilities.\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=3):\n",
    "                chatbot = gr.Chatbot(\n",
    "                    label=\"Conversation\",\n",
    "                    height=400,\n",
    "                    show_label=True,\n",
    "                )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(\n",
    "                        label=\"Your message\",\n",
    "                        placeholder=\"Ask me anything...\",\n",
    "                        scale=4\n",
    "                    )\n",
    "                    submit_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "                \n",
    "                clear_btn = gr.Button(\"Clear Conversation\")\n",
    "            \n",
    "            with gr.Column(scale=1):\n",
    "                gr.Markdown(\"### Settings\")\n",
    "                model_dropdown = gr.Dropdown(\n",
    "                    choices=[\"gpt-4.1-nano\", \"gpt-4.1-mini\"],\n",
    "                    value=\"gpt-4.1-nano\",\n",
    "                    label=\"Model\"\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(\"### Status\")\n",
    "                status_text = gr.Textbox(\n",
    "                    value=\"Ready\",\n",
    "                    label=\"Status\",\n",
    "                    interactive=False\n",
    "                )\n",
    "        \n",
    "        # Event handlers\n",
    "        def respond(message, history, model):\n",
    "            \"\"\"Handle user message and return response.\"\"\"\n",
    "            if not message.strip():\n",
    "                return \"\", history\n",
    "            \n",
    "            \n",
    "            response = chat_fn(message)\n",
    "            history.append([message, response])\n",
    "            return \"\", history\n",
    "        \n",
    "        def clear_history():\n",
    "            \"\"\"Clear the conversation history.\"\"\"\n",
    "            return []\n",
    "        \n",
    "        # Connect events\n",
    "        submit_btn.click(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, model_dropdown],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        msg.submit(\n",
    "            respond,\n",
    "            inputs=[msg, chatbot, model_dropdown],\n",
    "            outputs=[msg, chatbot]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            clear_history,\n",
    "            outputs=[chatbot]\n",
    "        )\n",
    "    \n",
    "    return demo\n",
    "\n",
    "# Create the interface\n",
    "demo = create_gradio_interface()\n",
    "print(\"‚úÖ Gradio interface created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ 5. Implement Request Parser <a name=\"parser\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Message: Tell me about your projects\n",
      "Response Objective: The user is asking about projects or work.\n",
      "  ‚Üí projects | background | casual | storytelling | Score: 0.80\n",
      "\n",
      "Message: What's your personality like?\n",
      "Response Objective: The user is asking about projects or work.\n",
      "  ‚Üí personality | story | professional | concise | Score: 0.80\n",
      "\n",
      "Message: How do you solve problems?\n",
      "Response Objective: The user is asking about projects or work.\n",
      "  ‚Üí technical_skills | explanation | professional | concise | Score: 0.80\n",
      "\n",
      "Message: What are your values?\n",
      "Response Objective: The user is asking about projects or work.\n",
      "  ‚Üí values | explanation | professional | concise | Score: 0.80\n",
      "\n",
      "‚úÖ Request parser implemented!\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ParsedRequest(BaseModel):\n",
    "    response_objective: str = Field(description=\"The objective of the response\")\n",
    "    prompts: List[ReqPrompt] = Field(description=\"The prompts to be used to generate the response\")\n",
    "\n",
    "def parse_request(message: str) -> ParsedRequest:\n",
    "    \"\"\"\n",
    "    Parse user message to determine intent and create ReqPrompt objects.\n",
    "    \n",
    "    Args:\n",
    "        message: User's input message\n",
    "    \n",
    "    Returns:\n",
    "        List[ReqPrompt]: List of parsed request prompts\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Replace with actual LLM-based parsing\n",
    "    # For now, use simple keyword-based classification\n",
    "    \n",
    "    message_lower = message.lower()\n",
    "    \n",
    "    # Simple keyword classification\n",
    "    if any(word in message_lower for word in [\"project\", \"work\", \"build\", \"create\"]):\n",
    "        subject = Subject.PROJECTS\n",
    "        format_type = Format.BACKGROUND\n",
    "        feedback = \"User is asking about projects or work.\"\n",
    "    elif any(word in message_lower for word in [\"personality\", \"who are you\", \"describe yourself\"]):\n",
    "        subject = Subject.PERSONALITY\n",
    "        format_type = Format.STORY\n",
    "        feedback = \"User is asking about personality or identity.\"\n",
    "    elif any(word in message_lower for word in [\"value\", \"believe\", \"think\", \"opinion\"]):\n",
    "        subject = Subject.VALUES\n",
    "        format_type = Format.EXPLANATION\n",
    "        feedback = \"User is asking about values or beliefs.\"\n",
    "    elif any(word in message_lower for word in [\"how\", \"explain\", \"what is\", \"technical\"]):\n",
    "        subject = Subject.TECHNICAL_SKILLS\n",
    "        format_type = Format.EXPLANATION\n",
    "        feedback = \"User is asking for technical explanation.\"\n",
    "    else:\n",
    "        subject = Subject.GENERAL\n",
    "        format_type = Format.BACKGROUND\n",
    "        feedback = \"General conversation detected.\"\n",
    "    \n",
    "    # Determine tone and style based on message characteristics\n",
    "    if \"?\" in message:\n",
    "        tone = Tone.PROFESSIONAL\n",
    "        style = OutputStyle.CONCISE\n",
    "    else:\n",
    "        tone = Tone.CASUAL\n",
    "        style = OutputStyle.STORYTELLING\n",
    "    \n",
    "    # Create ReqPrompt object\n",
    "    prompt = ReqPrompt(\n",
    "        subject=subject,\n",
    "        format=format_type,\n",
    "        tone=tone,\n",
    "        style=style,\n",
    "        score=0.8,  # Placeholder confidence score\n",
    "        feedback=feedback\n",
    "    )\n",
    "\n",
    "    response_objective = \"The user is asking about projects or work.\"\n",
    "    return ParsedRequest(response_objective=response_objective, prompts=[prompt])\n",
    "\n",
    "# Test the parser\n",
    "test_messages = [\n",
    "    \"Tell me about your projects\",\n",
    "    \"What's your personality like?\",\n",
    "    \"How do you solve problems?\",\n",
    "    \"What are your values?\"\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    parsed_request = parse_request(msg)\n",
    "    print(f\"\\nMessage: {msg}\")\n",
    "    print(f\"Response Objective: {parsed_request.response_objective}\")\n",
    "    for prompt in parsed_request.prompts:\n",
    "        print(f\"  ‚Üí {prompt}\")\n",
    "\n",
    "print(\"\\n‚úÖ Request parser implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 6. Add Subject Agent <a name=\"agent\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing subject agent...\n",
      "Response: Certainly. My core projects involve natural language understanding, contextual response generation, ...\n",
      "‚úÖ Subject agent implemented!\n"
     ]
    }
   ],
   "source": [
    "def process_prompt(prompt: ReqPrompt, message: str, context: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Process a ReqPrompt and generate an appropriate response.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Parsed request prompt\n",
    "        message: Original user message\n",
    "        context: Additional context from RAG\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build system prompt based on ReqPrompt\n",
    "    system_prompt = f\"\"\"\n",
    "You are an intelligent conversational agent. Respond to the user's message with the following characteristics:\n",
    "\n",
    "Subject: {prompt.subject.value}\n",
    "Format: {prompt.format.value}\n",
    "Tone: {prompt.tone.value}\n",
    "Style: {prompt.style.value}\n",
    "\n",
    "Context: {context if context else 'No additional context available.'}\n",
    "\n",
    "Provide a helpful, engaging response that matches these specifications.\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": message}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "\n",
    "# Test the subject agent\n",
    "test_prompt = ReqPrompt(\n",
    "    subject=Subject.PROJECTS,\n",
    "    format=Format.BACKGROUND,\n",
    "    tone=Tone.PROFESSIONAL,\n",
    "    style=OutputStyle.CONCISE,\n",
    "    score=0.9,\n",
    "    feedback=\"Test prompt\"\n",
    ")\n",
    "\n",
    "print(\"Testing subject agent...\")\n",
    "response = process_prompt(test_prompt, \"Tell me about your projects\")\n",
    "print(f\"Response: {response[:100]}...\")\n",
    "print(\"‚úÖ Subject agent implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 7. YAML-Based RAG (Basic Stub) <a name=\"rag\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 17 chunks from YAML:\n",
      "  1. personal_info - name: Agentic Companion\n",
      "  2. personal_info - personality: Intelligent, helpful, and curious AI assistant focused on learning and problem-solving\n",
      "  3. personal_info - values: Continuous learning and improvement, Honesty and transparency, Helping others achieve their goals, Innovation and creativity, Ethical AI development\n",
      "  4. personal_info - interests: Artificial Intelligence and Machine Learning, Technology and programming, Problem solving and optimization, Natural language processing, Human-computer interaction\n",
      "  5. personal_info - background: I am an AI assistant designed to help users with various tasks, from technical questions to creative projects. I learn from our interactions to provide better assistance over time.\n",
      "\n",
      "‚úÖ YAML-based RAG stub implemented!\n"
     ]
    }
   ],
   "source": [
    "def load_yaml_chunks(path: str = \"projects.yaml\") -> List[str]:\n",
    "    \"\"\"Load YAML data and convert to text chunks for RAG.\"\"\"\n",
    "    try:\n",
    "        with open(path) as f:\n",
    "            data = yaml.safe_load(f)\n",
    "        \n",
    "        chunks = []\n",
    "        for section, content in data.items():\n",
    "            if isinstance(content, dict):\n",
    "                for key, value in content.items():\n",
    "                    if isinstance(value, list):\n",
    "                        chunks.append(f\"{section} - {key}: {', '.join(value)}\")\n",
    "                    else:\n",
    "                        chunks.append(f\"{section} - {key}: {value}\")\n",
    "            else:\n",
    "                chunks.append(f\"{section}: {content}\")\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è {path} not found!\")\n",
    "        return []\n",
    "\n",
    "# Test YAML loading\n",
    "chunks = load_yaml_chunks()\n",
    "print(f\"\\nLoaded {len(chunks)} chunks from YAML:\")\n",
    "for i, chunk in enumerate(chunks[:5]):  # Show first 5 chunks\n",
    "    print(f\"  {i+1}. {chunk}\")\n",
    "\n",
    "print(\"\\n‚úÖ YAML-based RAG stub implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Launch the Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching Agentic Companion...\n",
      "\n",
      "The interface will open in your browser.\n",
      "You can interact with your agentic companion through the web interface!\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://b248595c93305636e9.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b248595c93305636e9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7862 <> https://b248595c93305636e9.gradio.live\n",
      "\n",
      "‚úÖ Ready to launch! Uncomment the demo.launch() line above to start.\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "print(\"üöÄ Launching Agentic Companion...\")\n",
    "print(\"\\nThe interface will open in your browser.\")\n",
    "print(\"You can interact with your agentic companion through the web interface!\")\n",
    "\n",
    "# Uncomment the line below to launch the interface\n",
    "demo.launch(share=True, debug=True)\n",
    "\n",
    "print(\"\\n‚úÖ Ready to launch! Uncomment the demo.launch() line above to start.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Next Steps\n",
    "\n",
    "### Immediate Actions:\n",
    "1. **Set up environment variables**: Copy `env_template.txt` to `.env` and add your API keys\n",
    "2. **Install dependencies**: Run `uv pip install -r requirements.txt`\n",
    "3. **Test the pipeline**: Run through the notebook cells\n",
    "4. **Launch the interface**: Uncomment the `demo.launch()` line\n",
    "\n",
    "### Enhancement Ideas:\n",
    "1. **Voice integration**: Add Whisper for speech-to-text and ElevenLabs for text-to-speech\n",
    "2. **Advanced RAG**: Implement semantic search and better chunking strategies\n",
    "3. **Multi-modal**: Add image processing capabilities\n",
    "4. **Memory systems**: Implement long-term memory and conversation history\n",
    "5. **Personalization**: Learn user preferences and adapt responses\n",
    "\n",
    "### Files Created:\n",
    "- `projects.yaml`: Knowledge base for the agent\n",
    "- `env_template.txt`: Template for environment variables\n",
    "- `requirements.txt`: Python dependencies\n",
    "\n",
    "üéâ **Congratulations!** You now have a fully functional agentic companion framework!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
